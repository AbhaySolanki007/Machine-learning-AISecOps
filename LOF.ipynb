{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load and parse JSON data from file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            content = f.read().strip()\n",
        "        if not content:\n",
        "            return []\n",
        "        if content.startswith(\"[\") and content.endswith(\"]\"):\n",
        "            return json.loads(content)\n",
        "        else:\n",
        "            content = re.sub(r\"}\\s*{\", \"},{\", content)\n",
        "            return json.loads(f\"[{content}]\")\n",
        "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "        print(f\"CRITICAL ERROR loading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def raw_feature_extraction(logs):\n",
        "    \"\"\"Raw feature extraction without any weights or attack-specific logic.\"\"\"\n",
        "\n",
        "    # Collect all unique syscalls across all logs\n",
        "    all_syscalls = set()\n",
        "    for log in logs:\n",
        "        syscalls = log.get(\"kernel\", {}).get(\"syscall_counts\", {})\n",
        "        all_syscalls.update(syscalls.keys())\n",
        "\n",
        "    # Sort syscalls for consistent feature order\n",
        "    all_syscalls = sorted(list(all_syscalls))\n",
        "    print(f\"  ‚Ä¢ Total unique syscalls found: {len(all_syscalls)}\")\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for log in logs:\n",
        "        syscalls = log.get(\"kernel\", {}).get(\"syscall_counts\", {})\n",
        "        total = max(1, sum(syscalls.values()))\n",
        "\n",
        "        # Create feature vector with raw syscall counts (normalized by total)\n",
        "        feature_vector = []\n",
        "        for syscall in all_syscalls:\n",
        "            count = syscalls.get(syscall, 0)\n",
        "            normalized_count = count / total\n",
        "            feature_vector.append(normalized_count)\n",
        "\n",
        "        # Add basic statistics without any attack-specific logic\n",
        "        unique_syscalls = len(syscalls)\n",
        "        total_activity = np.log1p(total)\n",
        "\n",
        "        # Add these as additional features\n",
        "        feature_vector.extend([unique_syscalls, total_activity])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Create column names\n",
        "    columns = all_syscalls + [\"unique_syscalls\", \"total_activity\"]\n",
        "\n",
        "    return pd.DataFrame(features, columns=columns)\n",
        "\n",
        "\n",
        "def evaluate_lof_results(y_true, lof_scores):\n",
        "    \"\"\"Evaluate LOF results with detailed classification breakdown.\"\"\"\n",
        "\n",
        "    # LOF returns negative scores for outliers (lower = more anomalous)\n",
        "    # We'll convert to positive scores where higher = more anomalous\n",
        "    anomaly_scores = -lof_scores\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    anomaly_scores = np.array(anomaly_scores)\n",
        "\n",
        "    # Calculate percentiles for threshold selection\n",
        "    attack_scores = anomaly_scores[y_true == 1]\n",
        "    normal_scores = anomaly_scores[y_true == 0]\n",
        "\n",
        "    print(f\"\\nüìä LOF Score Analysis:\")\n",
        "    print(\n",
        "        f\"  ‚Ä¢ Attack scores: min={attack_scores.min():.4f}, max={attack_scores.max():.4f}, mean={attack_scores.mean():.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  ‚Ä¢ Normal scores: min={normal_scores.min():.4f}, max={normal_scores.max():.4f}, mean={normal_scores.mean():.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  ‚Ä¢ Score separation: {np.mean(attack_scores) - np.mean(normal_scores):.4f}\"\n",
        "    )\n",
        "\n",
        "    # Simple threshold-based evaluation\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "    print(f\"\\nüîç LOF Threshold Analysis:\")\n",
        "    print(\n",
        "        f\"{'Threshold':<10} {'Detection Rate':<15} {'False Positive Rate':<20} {'Precision':<10}\"\n",
        "    )\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "    best_metrics = None\n",
        "    best_predictions = None\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Use percentile-based threshold\n",
        "        score_threshold = np.percentile(anomaly_scores, threshold * 100)\n",
        "\n",
        "        # Classify\n",
        "        predictions = (anomaly_scores >= score_threshold).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        tp = np.sum((y_true == 1) & (predictions == 1))\n",
        "        fp = np.sum((y_true == 0) & (predictions == 1))\n",
        "        tn = np.sum((y_true == 0) & (predictions == 0))\n",
        "        fn = np.sum((y_true == 1) & (predictions == 0))\n",
        "\n",
        "        detection_rate = tp / max(1, tp + fn)\n",
        "        false_positive_rate = fp / max(1, fp + tn)\n",
        "        precision = tp / max(1, tp + fp)\n",
        "        f1_score = (\n",
        "            2 * (precision * detection_rate) / max(0.001, precision + detection_rate)\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"{threshold:<10} {detection_rate:<15.3f} {false_positive_rate:<20.3f} {precision:<10.3f}\"\n",
        "        )\n",
        "\n",
        "        if f1_score > best_f1:\n",
        "            best_f1 = f1_score\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                \"threshold\": score_threshold,\n",
        "                \"detection_rate\": detection_rate,\n",
        "                \"false_positive_rate\": false_positive_rate,\n",
        "                \"precision\": precision,\n",
        "                \"f1_score\": f1_score,\n",
        "            }\n",
        "            best_predictions = predictions\n",
        "\n",
        "    # Detailed classification breakdown\n",
        "    if best_predictions is not None:\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"DETAILED CLASSIFICATION BREAKDOWN\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Calculate detailed metrics\n",
        "        attacks_as_attack = np.sum((y_true == 1) & (best_predictions == 1))\n",
        "        attacks_as_safe = np.sum((y_true == 1) & (best_predictions == 0))\n",
        "\n",
        "        normal_as_attack = np.sum((y_true == 0) & (best_predictions == 1))\n",
        "        normal_as_safe = np.sum((y_true == 0) & (best_predictions == 0))\n",
        "\n",
        "        total_attacks = np.sum(y_true == 1)\n",
        "        total_normal = np.sum(y_true == 0)\n",
        "\n",
        "        print(f\"\\nüéØ ATTACK LOGS ({total_attacks} total):\")\n",
        "        print(\n",
        "            f\"  ‚úì Flagged as ATTACK:     {attacks_as_attack:4d} ({(attacks_as_attack/total_attacks):.1%})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  ‚úó Missed (flagged SAFE): {attacks_as_safe:4d} ({(attacks_as_safe/total_attacks):.1%})\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\nüõ°Ô∏è NORMAL LOGS ({total_normal} total):\")\n",
        "        print(\n",
        "            f\"  ‚úó False ATTACK flags:    {normal_as_attack:4d} ({(normal_as_attack/total_normal):.1%})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  ‚úì Correctly SAFE:        {normal_as_safe:4d} ({(normal_as_safe/total_normal):.1%})\"\n",
        "        )\n",
        "\n",
        "        # Performance metrics\n",
        "        print(f\"\\nüìà PERFORMANCE METRICS:\")\n",
        "        print(f\"  ‚Ä¢ Attack Detection Rate: {best_metrics['detection_rate']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ Critical Miss Rate: {(attacks_as_safe/total_attacks):.1%}\")\n",
        "        print(f\"  ‚Ä¢ False Positive Rate: {(normal_as_attack/total_normal):.1%}\")\n",
        "        print(f\"  ‚Ä¢ Precision: {best_metrics['precision']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "\n",
        "    return best_metrics, anomaly_scores\n",
        "\n",
        "\n",
        "def run_pure_lof_analysis():\n",
        "    \"\"\"Run pure LOF analysis without any attack-specific logic.\"\"\"\n",
        "\n",
        "    # File paths\n",
        "    train_file = \"/content/training_data_kernel_activity.json\"\n",
        "    attack_file = \"/content/all_attacks.json\"\n",
        "    validation_file = \"/content/normal_validation.json\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"PURE LOF ANOMALY DETECTION (NO WEIGHTS)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\n[1/4] Loading datasets...\")\n",
        "    train_data = load_data(train_file)\n",
        "    attack_data = load_data(attack_file)\n",
        "    normal_validation_data = load_data(validation_file)\n",
        "\n",
        "    print(f\"  ‚Ä¢ Training samples: {len(train_data)}\")\n",
        "    print(f\"  ‚Ä¢ Attack samples: {len(attack_data)}\")\n",
        "    print(f\"  ‚Ä¢ Normal validation samples: {len(normal_validation_data)}\")\n",
        "\n",
        "    # Check for empty datasets\n",
        "    if (\n",
        "        len(train_data) == 0\n",
        "        or len(attack_data) == 0\n",
        "        or len(normal_validation_data) == 0\n",
        "    ):\n",
        "        print(\"  ‚ùå ERROR: One or more datasets are empty!\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n[2/4] Extracting raw features (no weights)...\")\n",
        "    # Combine all data to get complete syscall vocabulary\n",
        "    all_data = train_data + attack_data + normal_validation_data\n",
        "    X_all_df = raw_feature_extraction(all_data)\n",
        "\n",
        "    # Split back into datasets\n",
        "    X_train_df = X_all_df.iloc[: len(train_data)]\n",
        "    X_attack_df = X_all_df.iloc[len(train_data) : len(train_data) + len(attack_data)]\n",
        "    X_normal_val_df = X_all_df.iloc[len(train_data) + len(attack_data) :]\n",
        "\n",
        "    print(f\"  ‚Ä¢ Feature dimensions: {X_train_df.shape[1]}\")\n",
        "    print(f\"  ‚Ä¢ Features: Raw syscall counts + basic stats\")\n",
        "\n",
        "    print(\"\\n[3/4] Training pure LOF model...\")\n",
        "    # Scale features\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
        "\n",
        "    # Train LOF on normal data only\n",
        "    lof = LocalOutlierFactor(\n",
        "        n_neighbors=20,\n",
        "        contamination=0.1,\n",
        "        novelty=False,  # Use fit_predict for training data\n",
        "        metric=\"minkowski\",\n",
        "        p=2,\n",
        "    )\n",
        "\n",
        "    # Fit and predict on training data\n",
        "    lof_scores_train = lof.fit_predict(X_train_scaled)\n",
        "\n",
        "    print(f\"  ‚Ä¢ LOF model trained with {lof.n_neighbors_} neighbors\")\n",
        "    print(f\"  ‚Ä¢ Contamination estimate: {lof.contamination}\")\n",
        "\n",
        "    print(\"\\n[4/4] Evaluating on validation data...\")\n",
        "    # Prepare combined validation data\n",
        "    X_combined_df = pd.concat([X_attack_df, X_normal_val_df], ignore_index=True)\n",
        "    y_true = np.array([1] * len(X_attack_df) + [0] * len(X_normal_val_df))\n",
        "\n",
        "    # Scale validation data\n",
        "    X_combined_scaled = scaler.transform(X_combined_df)\n",
        "\n",
        "    # For novelty=False, we need to fit on the combined data to get scores\n",
        "    lof_combined = LocalOutlierFactor(\n",
        "        n_neighbors=20, contamination=0.1, novelty=False, metric=\"minkowski\", p=2\n",
        "    )\n",
        "\n",
        "    # Fit on combined data to get outlier factors\n",
        "    lof_combined.fit(X_combined_scaled)\n",
        "    lof_scores_val = lof_combined.negative_outlier_factor_\n",
        "\n",
        "    # Evaluate results\n",
        "    best_metrics, anomaly_scores = evaluate_lof_results(y_true, lof_scores_val)\n",
        "\n",
        "    if best_metrics:\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"PURE LOF RESULTS SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"\\nüéØ Best Performance (Threshold: {best_metrics['threshold']:.4f}):\")\n",
        "        print(f\"  ‚Ä¢ Detection Rate: {best_metrics['detection_rate']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ False Positive Rate: {best_metrics['false_positive_rate']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ Precision: {best_metrics['precision']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        attack_scores = anomaly_scores[y_true == 1]\n",
        "        normal_scores = anomaly_scores[y_true == 0]\n",
        "\n",
        "        print(f\"\\nüìä Score Distribution:\")\n",
        "        print(\n",
        "            f\"  ‚Ä¢ Attack mean: {np.mean(attack_scores):.4f} (std: {np.std(attack_scores):.4f})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  ‚Ä¢ Normal mean: {np.mean(normal_scores):.4f} (std: {np.std(normal_scores):.4f})\"\n",
        "        )\n",
        "        print(f\"  ‚Ä¢ Separation: {np.mean(attack_scores) - np.mean(normal_scores):.4f}\")\n",
        "\n",
        "        if np.mean(attack_scores) > np.mean(normal_scores):\n",
        "            print(f\"  ‚úÖ Good separation: Attacks score higher than normals\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è Poor separation: Attacks score lower than normals\")\n",
        "\n",
        "    return best_metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nüöÄ Starting Pure LOF Analysis (No Weights)...\\n\")\n",
        "\n",
        "    metrics = run_pure_lof_analysis()\n",
        "\n",
        "    if metrics is None:\n",
        "        print(\"\\n‚ùå Analysis failed due to data loading issues.\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PURE LOF ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H70Jpxzn2cjY",
        "outputId": "d638c7b4-3626-47a1-e8b2-8b11f46964e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting Pure LOF Analysis (No Weights)...\n",
            "\n",
            "============================================================\n",
            "PURE LOF ANOMALY DETECTION (NO WEIGHTS)\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading datasets...\n",
            "  ‚Ä¢ Training samples: 833\n",
            "  ‚Ä¢ Attack samples: 746\n",
            "  ‚Ä¢ Normal validation samples: 4372\n",
            "\n",
            "[2/4] Extracting raw features (no weights)...\n",
            "  ‚Ä¢ Total unique syscalls found: 175\n",
            "  ‚Ä¢ Feature dimensions: 177\n",
            "  ‚Ä¢ Features: Raw syscall counts + basic stats\n",
            "\n",
            "[3/4] Training pure LOF model...\n",
            "  ‚Ä¢ LOF model trained with 20 neighbors\n",
            "  ‚Ä¢ Contamination estimate: 0.1\n",
            "\n",
            "[4/4] Evaluating on validation data...\n",
            "\n",
            "üìä LOF Score Analysis:\n",
            "  ‚Ä¢ Attack scores: min=0.9525, max=38742628045.1471, mean=139800126.4651\n",
            "  ‚Ä¢ Normal scores: min=0.9262, max=97397703238.9581, mean=592135681.9474\n",
            "  ‚Ä¢ Score separation: -452335555.4823\n",
            "\n",
            "üîç LOF Threshold Analysis:\n",
            "Threshold  Detection Rate  False Positive Rate  Precision \n",
            "------------------------------------------------------------\n",
            "0.5        0.660           0.473                0.192     \n",
            "0.6        0.477           0.387                0.174     \n",
            "0.7        0.269           0.305                0.131     \n",
            "0.8        0.166           0.206                0.121     \n",
            "0.9        0.068           0.105                0.100     \n",
            "\n",
            "============================================================\n",
            "DETAILED CLASSIFICATION BREAKDOWN\n",
            "============================================================\n",
            "\n",
            "üéØ ATTACK LOGS (746 total):\n",
            "  ‚úì Flagged as ATTACK:      492 (66.0%)\n",
            "  ‚úó Missed (flagged SAFE):  254 (34.0%)\n",
            "\n",
            "üõ°Ô∏è NORMAL LOGS (4372 total):\n",
            "  ‚úó False ATTACK flags:    2067 (47.3%)\n",
            "  ‚úì Correctly SAFE:        2305 (52.7%)\n",
            "\n",
            "üìà PERFORMANCE METRICS:\n",
            "  ‚Ä¢ Attack Detection Rate: 66.0%\n",
            "  ‚Ä¢ Critical Miss Rate: 34.0%\n",
            "  ‚Ä¢ False Positive Rate: 47.3%\n",
            "  ‚Ä¢ Precision: 19.2%\n",
            "  ‚Ä¢ F1 Score: 0.298\n",
            "\n",
            "============================================================\n",
            "PURE LOF RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "üéØ Best Performance (Threshold: 1.0236):\n",
            "  ‚Ä¢ Detection Rate: 66.0%\n",
            "  ‚Ä¢ False Positive Rate: 47.3%\n",
            "  ‚Ä¢ Precision: 19.2%\n",
            "  ‚Ä¢ F1 Score: 0.298\n",
            "\n",
            "üìä Score Distribution:\n",
            "  ‚Ä¢ Attack mean: 139800126.4651 (std: 1933096597.9077)\n",
            "  ‚Ä¢ Normal mean: 592135681.9474 (std: 6042583227.8718)\n",
            "  ‚Ä¢ Separation: -452335555.4823\n",
            "  ‚ö†Ô∏è Poor separation: Attacks score lower than normals\n",
            "\n",
            "============================================================\n",
            "PURE LOF ANALYSIS COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sulDAB2z1zyc",
        "outputId": "d8cb2344-d8e8-47c1-8a9c-e512d6e46d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting Simple LOF Analysis...\n",
            "\n",
            "============================================================\n",
            "SIMPLE LOF ANOMALY DETECTION\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading datasets...\n",
            "  ‚Ä¢ Training samples: 833\n",
            "  ‚Ä¢ Attack samples: 746\n",
            "  ‚Ä¢ Normal validation samples: 4372\n",
            "\n",
            "[2/4] Extracting simple features...\n",
            "  ‚Ä¢ Feature dimensions: 11\n",
            "  ‚Ä¢ Features: attack_score, has_ptrace, has_process_vm, has_execve, has_setuid, critical_intensity, execution_intensity, network_intensity, rare_ratio, unique_syscalls, total_activity\n",
            "\n",
            "[3/4] Training LOF model...\n",
            "  ‚Ä¢ LOF model trained with 20 neighbors\n",
            "  ‚Ä¢ Contamination estimate: 0.1\n",
            "\n",
            "[4/4] Evaluating on validation data...\n",
            "\n",
            "üìä LOF Score Analysis:\n",
            "  ‚Ä¢ Attack scores: min=0.9411, max=7769311824.0763, mean=10621604.7631\n",
            "  ‚Ä¢ Normal scores: min=0.9336, max=7899405402.8009, mean=68864685.5493\n",
            "  ‚Ä¢ Score separation: -58243080.7863\n",
            "\n",
            "üîç LOF Threshold Analysis:\n",
            "Threshold  Detection Rate  False Positive Rate  Precision \n",
            "------------------------------------------------------------\n",
            "0.5        0.591           0.484                0.172     \n",
            "0.6        0.445           0.392                0.162     \n",
            "0.7        0.295           0.301                0.143     \n",
            "0.8        0.177           0.204                0.129     \n",
            "0.9        0.062           0.107                0.090     \n",
            "\n",
            "============================================================\n",
            "DETAILED CLASSIFICATION BREAKDOWN\n",
            "============================================================\n",
            "\n",
            "üéØ ATTACK LOGS (746 total):\n",
            "  ‚úì Flagged as ATTACK:      441 (59.1%)\n",
            "  ‚úó Missed (flagged SAFE):  305 (40.9%)\n",
            "\n",
            "üõ°Ô∏è NORMAL LOGS (4372 total):\n",
            "  ‚úó False ATTACK flags:    2118 (48.4%)\n",
            "  ‚úì Correctly SAFE:        2254 (51.6%)\n",
            "\n",
            "üìà PERFORMANCE METRICS:\n",
            "  ‚Ä¢ Attack Detection Rate: 59.1%\n",
            "  ‚Ä¢ Critical Miss Rate: 40.9%\n",
            "  ‚Ä¢ False Positive Rate: 48.4%\n",
            "  ‚Ä¢ Precision: 17.2%\n",
            "  ‚Ä¢ F1 Score: 0.267\n",
            "\n",
            "============================================================\n",
            "SIMPLE LOF RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "üéØ Best Performance (Threshold: 1.0171):\n",
            "  ‚Ä¢ Detection Rate: 59.1%\n",
            "  ‚Ä¢ False Positive Rate: 48.4%\n",
            "  ‚Ä¢ Precision: 17.2%\n",
            "  ‚Ä¢ F1 Score: 0.267\n",
            "\n",
            "üìä Score Distribution:\n",
            "  ‚Ä¢ Attack mean: 10621604.7631 (std: 284312548.3396)\n",
            "  ‚Ä¢ Normal mean: 68864685.5493 (std: 412613279.5378)\n",
            "  ‚Ä¢ Separation: -58243080.7863\n",
            "  ‚ö†Ô∏è Poor separation: Attacks score lower than normals\n",
            "\n",
            "============================================================\n",
            "SIMPLE LOF ANALYSIS COMPLETE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load and parse JSON data from file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            content = f.read().strip()\n",
        "        if not content:\n",
        "            return []\n",
        "        if content.startswith(\"[\") and content.endswith(\"]\"):\n",
        "            return json.loads(content)\n",
        "        else:\n",
        "            content = re.sub(r\"}\\s*{\", \"},{\", content)\n",
        "            return json.loads(f\"[{content}]\")\n",
        "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "        print(f\"CRITICAL ERROR loading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def simple_feature_extraction(logs):\n",
        "    \"\"\"Simple feature extraction for LOF analysis.\"\"\"\n",
        "\n",
        "    # Basic syscall weights for attack detection\n",
        "    attack_indicators = {\n",
        "        \"ptrace\": 10.0,\n",
        "        \"process_vm_readv\": 10.0,\n",
        "        \"process_vm_writev\": 10.0,\n",
        "        \"kexec_load\": 8.0,\n",
        "        \"init_module\": 8.0,\n",
        "        \"delete_module\": 6.0,\n",
        "        \"capset\": 5.0,\n",
        "        \"setuid\": 4.0,\n",
        "        \"setgid\": 4.0,\n",
        "        \"execve\": 3.0,\n",
        "        \"clone\": 2.0,\n",
        "        \"fork\": 1.0,\n",
        "        \"socket\": 2.0,\n",
        "        \"connect\": 2.0,\n",
        "        \"mprotect\": 3.0,\n",
        "        \"mmap\": 2.0,\n",
        "    }\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for log in logs:\n",
        "        syscalls = log.get(\"kernel\", {}).get(\"syscall_counts\", {})\n",
        "        total = max(1, sum(syscalls.values()))\n",
        "\n",
        "        # Basic attack score\n",
        "        attack_score = 0\n",
        "        for sc, cnt in syscalls.items():\n",
        "            weight = attack_indicators.get(sc, 0.5)\n",
        "            attack_score += cnt * weight\n",
        "\n",
        "        normalized_attack_score = attack_score / total\n",
        "\n",
        "        # Simple pattern features\n",
        "        has_ptrace = 1 if \"ptrace\" in syscalls else 0\n",
        "        has_process_vm = (\n",
        "            1\n",
        "            if any(sc in syscalls for sc in [\"process_vm_readv\", \"process_vm_writev\"])\n",
        "            else 0\n",
        "        )\n",
        "        has_execve = 1 if \"execve\" in syscalls else 0\n",
        "        has_setuid = 1 if \"setuid\" in syscalls else 0\n",
        "\n",
        "        # Basic intensities\n",
        "        critical_intensity = (\n",
        "            sum(\n",
        "                syscalls.get(sc, 0)\n",
        "                for sc in [\n",
        "                    \"ptrace\",\n",
        "                    \"process_vm_readv\",\n",
        "                    \"process_vm_writev\",\n",
        "                    \"kexec_load\",\n",
        "                ]\n",
        "            )\n",
        "            / total\n",
        "        )\n",
        "        execution_intensity = (\n",
        "            sum(syscalls.get(sc, 0) for sc in [\"execve\", \"clone\", \"fork\"]) / total\n",
        "        )\n",
        "        network_intensity = (\n",
        "            sum(syscalls.get(sc, 0) for sc in [\"socket\", \"connect\", \"bind\"]) / total\n",
        "        )\n",
        "\n",
        "        # Rare syscalls\n",
        "        rare_syscalls = [sc for sc in syscalls if sc.startswith(\"syscall_\")]\n",
        "        rare_ratio = sum(syscalls.get(sc, 0) for sc in rare_syscalls) / total\n",
        "\n",
        "        # Basic features\n",
        "        unique_syscalls = len(syscalls)\n",
        "        total_activity = np.log1p(total)\n",
        "\n",
        "        features.append(\n",
        "            [\n",
        "                normalized_attack_score,\n",
        "                has_ptrace,\n",
        "                has_process_vm,\n",
        "                has_execve,\n",
        "                has_setuid,\n",
        "                critical_intensity,\n",
        "                execution_intensity,\n",
        "                network_intensity,\n",
        "                rare_ratio,\n",
        "                unique_syscalls,\n",
        "                total_activity,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    columns = [\n",
        "        \"attack_score\",\n",
        "        \"has_ptrace\",\n",
        "        \"has_process_vm\",\n",
        "        \"has_execve\",\n",
        "        \"has_setuid\",\n",
        "        \"critical_intensity\",\n",
        "        \"execution_intensity\",\n",
        "        \"network_intensity\",\n",
        "        \"rare_ratio\",\n",
        "        \"unique_syscalls\",\n",
        "        \"total_activity\",\n",
        "    ]\n",
        "\n",
        "    return pd.DataFrame(features, columns=columns)\n",
        "\n",
        "\n",
        "def evaluate_lof_results(y_true, lof_scores):\n",
        "    \"\"\"Evaluate LOF results with basic metrics.\"\"\"\n",
        "\n",
        "    # LOF returns negative scores for outliers (lower = more anomalous)\n",
        "    # We'll convert to positive scores where higher = more anomalous\n",
        "    anomaly_scores = -lof_scores\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    anomaly_scores = np.array(anomaly_scores)\n",
        "\n",
        "    # Calculate percentiles for threshold selection\n",
        "    attack_scores = anomaly_scores[y_true == 1]\n",
        "    normal_scores = anomaly_scores[y_true == 0]\n",
        "\n",
        "    print(f\"\\nüìä LOF Score Analysis:\")\n",
        "    print(\n",
        "        f\"  ‚Ä¢ Attack scores: min={attack_scores.min():.4f}, max={attack_scores.max():.4f}, mean={attack_scores.mean():.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  ‚Ä¢ Normal scores: min={normal_scores.min():.4f}, max={normal_scores.max():.4f}, mean={normal_scores.mean():.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"  ‚Ä¢ Score separation: {np.mean(attack_scores) - np.mean(normal_scores):.4f}\"\n",
        "    )\n",
        "\n",
        "    # Simple threshold-based evaluation\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "    print(f\"\\nüîç LOF Threshold Analysis:\")\n",
        "    print(\n",
        "        f\"{'Threshold':<10} {'Detection Rate':<15} {'False Positive Rate':<20} {'Precision':<10}\"\n",
        "    )\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "    best_metrics = None\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Use percentile-based threshold\n",
        "        score_threshold = np.percentile(anomaly_scores, threshold * 100)\n",
        "\n",
        "        # Classify\n",
        "        predictions = (anomaly_scores >= score_threshold).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        tp = np.sum((y_true == 1) & (predictions == 1))\n",
        "        fp = np.sum((y_true == 0) & (predictions == 1))\n",
        "        tn = np.sum((y_true == 0) & (predictions == 0))\n",
        "        fn = np.sum((y_true == 1) & (predictions == 0))\n",
        "\n",
        "        detection_rate = tp / max(1, tp + fn)\n",
        "        false_positive_rate = fp / max(1, fp + tn)\n",
        "        precision = tp / max(1, tp + fp)\n",
        "        f1_score = (\n",
        "            2 * (precision * detection_rate) / max(0.001, precision + detection_rate)\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"{threshold:<10} {detection_rate:<15.3f} {false_positive_rate:<20.3f} {precision:<10.3f}\"\n",
        "        )\n",
        "\n",
        "        if f1_score > best_f1:\n",
        "            best_f1 = f1_score\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                \"threshold\": score_threshold,\n",
        "                \"detection_rate\": detection_rate,\n",
        "                \"false_positive_rate\": false_positive_rate,\n",
        "                \"precision\": precision,\n",
        "                \"f1_score\": f1_score,\n",
        "            }\n",
        "            best_predictions = predictions\n",
        "\n",
        "    # Detailed classification breakdown\n",
        "    if best_predictions is not None:\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"DETAILED CLASSIFICATION BREAKDOWN\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Calculate detailed metrics\n",
        "        attacks_as_attack = np.sum((y_true == 1) & (best_predictions == 1))\n",
        "        attacks_as_safe = np.sum((y_true == 1) & (best_predictions == 0))\n",
        "\n",
        "        normal_as_attack = np.sum((y_true == 0) & (best_predictions == 1))\n",
        "        normal_as_safe = np.sum((y_true == 0) & (best_predictions == 0))\n",
        "\n",
        "        total_attacks = np.sum(y_true == 1)\n",
        "        total_normal = np.sum(y_true == 0)\n",
        "\n",
        "        print(f\"\\nüéØ ATTACK LOGS ({total_attacks} total):\")\n",
        "        print(\n",
        "            f\"  ‚úì Flagged as ATTACK:     {attacks_as_attack:4d} ({(attacks_as_attack/total_attacks):.1%})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  ‚úó Missed (flagged SAFE): {attacks_as_safe:4d} ({(attacks_as_safe/total_attacks):.1%})\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\nüõ°Ô∏è NORMAL LOGS ({total_normal} total):\")\n",
        "        print(\n",
        "            f\"  ‚úó False ATTACK flags:    {normal_as_attack:4d} ({(normal_as_attack/total_normal):.1%})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  ‚úì Correctly SAFE:        {normal_as_safe:4d} ({(normal_as_safe/total_normal):.1%})\"\n",
        "        )\n",
        "\n",
        "        # Performance metrics\n",
        "        print(f\"\\nüìà PERFORMANCE METRICS:\")\n",
        "        print(f\"  ‚Ä¢ Attack Detection Rate: {best_metrics['detection_rate']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ Critical Miss Rate: {(attacks_as_safe/total_attacks):.1%}\")\n",
        "        print(f\"  ‚Ä¢ False Positive Rate: {(normal_as_attack/total_normal):.1%}\")\n",
        "        print(f\"  ‚Ä¢ Precision: {best_metrics['precision']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "\n",
        "    return best_metrics, anomaly_scores\n",
        "\n",
        "\n",
        "def run_simple_lof_analysis():\n",
        "    \"\"\"Run simple LOF analysis on the dataset.\"\"\"\n",
        "\n",
        "    # File paths\n",
        "    train_file = \"/content/training_data_kernel_activity.json\"\n",
        "    attack_file = \"/content/all_attacks.json\"\n",
        "    validation_file = \"/content/normal_validation.json\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SIMPLE LOF ANOMALY DETECTION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\n[1/4] Loading datasets...\")\n",
        "    train_data = load_data(train_file)\n",
        "    attack_data = load_data(attack_file)\n",
        "    normal_validation_data = load_data(validation_file)\n",
        "\n",
        "    print(f\"  ‚Ä¢ Training samples: {len(train_data)}\")\n",
        "    print(f\"  ‚Ä¢ Attack samples: {len(attack_data)}\")\n",
        "    print(f\"  ‚Ä¢ Normal validation samples: {len(normal_validation_data)}\")\n",
        "\n",
        "    # Check for empty datasets\n",
        "    if (\n",
        "        len(train_data) == 0\n",
        "        or len(attack_data) == 0\n",
        "        or len(normal_validation_data) == 0\n",
        "    ):\n",
        "        print(\"  ‚ùå ERROR: One or more datasets are empty!\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n[2/4] Extracting simple features...\")\n",
        "    X_train_df = simple_feature_extraction(train_data)\n",
        "    X_attack_df = simple_feature_extraction(attack_data)\n",
        "    X_normal_val_df = simple_feature_extraction(normal_validation_data)\n",
        "\n",
        "    print(f\"  ‚Ä¢ Feature dimensions: {X_train_df.shape[1]}\")\n",
        "    print(f\"  ‚Ä¢ Features: {', '.join(X_train_df.columns)}\")\n",
        "\n",
        "    print(\"\\n[3/4] Training LOF model...\")\n",
        "    # Scale features\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
        "\n",
        "    # Train LOF on normal data only\n",
        "    lof = LocalOutlierFactor(\n",
        "        n_neighbors=20,\n",
        "        contamination=0.1,\n",
        "        novelty=False,  # Use fit_predict for training data\n",
        "        metric=\"minkowski\",\n",
        "        p=2,\n",
        "    )\n",
        "\n",
        "    # Fit and predict on training data\n",
        "    lof_scores_train = lof.fit_predict(X_train_scaled)\n",
        "\n",
        "    print(f\"  ‚Ä¢ LOF model trained with {lof.n_neighbors_} neighbors\")\n",
        "    print(f\"  ‚Ä¢ Contamination estimate: {lof.contamination}\")\n",
        "\n",
        "    print(\"\\n[4/4] Evaluating on validation data...\")\n",
        "    # Prepare combined validation data\n",
        "    X_combined_df = pd.concat([X_attack_df, X_normal_val_df], ignore_index=True)\n",
        "    y_true = np.array([1] * len(X_attack_df) + [0] * len(X_normal_val_df))\n",
        "\n",
        "    # Scale validation data\n",
        "    X_combined_scaled = scaler.transform(X_combined_df)\n",
        "\n",
        "    # For novelty=False, we need to fit on the combined data to get scores\n",
        "    # We'll use the negative_outlier_factor_ from the training data as reference\n",
        "    lof_combined = LocalOutlierFactor(\n",
        "        n_neighbors=20, contamination=0.1, novelty=False, metric=\"minkowski\", p=2\n",
        "    )\n",
        "\n",
        "    # Fit on combined data to get outlier factors\n",
        "    lof_combined.fit(X_combined_scaled)\n",
        "    lof_scores_val = lof_combined.negative_outlier_factor_\n",
        "\n",
        "    # Evaluate results\n",
        "    best_metrics, anomaly_scores = evaluate_lof_results(y_true, lof_scores_val)\n",
        "\n",
        "    if best_metrics:\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"SIMPLE LOF RESULTS SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"\\nüéØ Best Performance (Threshold: {best_metrics['threshold']:.4f}):\")\n",
        "        print(f\"  ‚Ä¢ Detection Rate: {best_metrics['detection_rate']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ False Positive Rate: {best_metrics['false_positive_rate']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ Precision: {best_metrics['precision']:.1%}\")\n",
        "        print(f\"  ‚Ä¢ F1 Score: {best_metrics['f1_score']:.3f}\")\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        attack_scores = anomaly_scores[y_true == 1]\n",
        "        normal_scores = anomaly_scores[y_true == 0]\n",
        "\n",
        "        print(f\"\\nüìä Score Distribution:\")\n",
        "        print(\n",
        "            f\"  ‚Ä¢ Attack mean: {np.mean(attack_scores):.4f} (std: {np.std(attack_scores):.4f})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  ‚Ä¢ Normal mean: {np.mean(normal_scores):.4f} (std: {np.std(normal_scores):.4f})\"\n",
        "        )\n",
        "        print(f\"  ‚Ä¢ Separation: {np.mean(attack_scores) - np.mean(normal_scores):.4f}\")\n",
        "\n",
        "        if np.mean(attack_scores) > np.mean(normal_scores):\n",
        "            print(f\"  ‚úÖ Good separation: Attacks score higher than normals\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è Poor separation: Attacks score lower than normals\")\n",
        "\n",
        "    return best_metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nüöÄ Starting Simple LOF Analysis...\\n\")\n",
        "\n",
        "    metrics = run_simple_lof_analysis()\n",
        "\n",
        "    if metrics is None:\n",
        "        print(\"\\n‚ùå Analysis failed due to data loading issues.\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SIMPLE LOF ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 60)\n"
      ]
    }
  ]
}